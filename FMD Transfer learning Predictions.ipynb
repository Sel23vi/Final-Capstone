{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mKoSiS2QEGL","executionInfo":{"status":"ok","timestamp":1722237750006,"user_tz":-330,"elapsed":21980,"user":{"displayName":"Anju Doraisamy","userId":"07553379767866067180"}},"outputId":"ba65d68a-97cb-418d-a337-3c31dc606f2c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import zipfile\n","zip_file_path = '/content/drive/MyDrive/Pred Img.zip'\n","extract_path = '/content/Img'\n","\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)"],"metadata":{"id":"GOm5iN0zU3zq","executionInfo":{"status":"ok","timestamp":1722229946712,"user_tz":-330,"elapsed":3160,"user":{"displayName":"Anju Doraisamy","userId":"07553379767866067180"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["! pip install mtcnn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDRsVMtpXSWT","executionInfo":{"status":"ok","timestamp":1722237692406,"user_tz":-330,"elapsed":4680,"user":{"displayName":"Anju Doraisamy","userId":"07553379767866067180"}},"outputId":"4f3ce423-13b3-453d-d3bb-061f5875ed0c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mtcnn\n","  Downloading mtcnn-0.1.1-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.15.0)\n","Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.10.0.84)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.25.2)\n","Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: mtcnn\n","Successfully installed mtcnn-0.1.1\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from mtcnn import MTCNN\n","\n","# Function to preprocess face image for prediction\n","def preprocess_face(face_img, target_size=(224, 224)):\n","    face_img = cv2.resize(face_img, target_size)\n","    face_img = face_img / 255.0  # Normalize to [0, 1]\n","    face_img = np.expand_dims(face_img, axis=0)  # Add batch dimension\n","    return face_img\n","\n","# Function to expand the bounding box\n","def expand_bbox(bbox, expansion_factor=0.3):\n","    x, y, w, h = bbox\n","    new_w = int(w * (1 + expansion_factor))\n","    new_h = int(h * (1 + expansion_factor))\n","    new_x = max(0, x - (new_w - w) // 2)\n","    new_y = max(0, y - (new_h - h) // 2)\n","    return new_x, new_y, new_w, new_h\n","\n","# Load the trained model\n","model_path = '/content/drive/My Drive/face_mask_detection_model_1pt4.h5'  # Update with the actual path to your model\n","model = tf.keras.models.load_model(model_path)\n","\n","# Class names\n","class_names = ['with_mask', 'without_mask']\n","\n","# Initialize the face detector with adjusted parameters\n","detector = MTCNN(\n","    min_face_size=15,  # Adjust as needed\n","    scale_factor=0.5,  # Adjust as needed\n","    steps_threshold=[0.4, 0.4, 0.3]  # Lower values for more sensitivity\n",")\n","\n","# Define the folder path\n","folder_path = '/content/Img/Pred Img/'\n","\n","# List to store images and labels\n","images_with_predictions = []\n","\n","# Loop through all images in the folder\n","for filename in os.listdir(folder_path):\n","    if filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n","        img_path = os.path.join(folder_path, filename)\n","        img = cv2.imread(img_path)\n","\n","        if img is not None:\n","            # Detect faces in the image\n","            faces = detector.detect_faces(img)\n","            for face in faces:\n","                x, y, w, h = face['box']\n","                x, y, w, h = expand_bbox((x, y, w, h), expansion_factor=0.3)  # Adjust the expansion factor as needed\n","                face_img = img[y:y+h, x:x+w]\n","\n","                if face_img.size == 0:\n","                    continue\n","\n","                # Preprocess the face image\n","                face_img = preprocess_face(face_img)\n","\n","                # Predict using the trained model\n","                pred = model.predict(face_img)\n","                confidence = pred[0][0]\n","                class_idx = int(confidence > 0.5)\n","                class_name = class_names[class_idx]\n","\n","                # Draw bounding box and label on the image\n","                color = (0, 255, 0) if class_idx == 0 else (0, 0, 255)\n","                label = f\"{class_name}: {confidence:.2f}\"\n","                cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)\n","                cv2.putText(img, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n","\n","            # Convert BGR image to RGB for displaying with matplotlib\n","            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            images_with_predictions.append((img_rgb, label))\n","\n","# Display the images in a grid\n","def plot_images(images, labels, cols=4, rows=3):\n","    fig, axes = plt.subplots(rows, cols, figsize=(20, 15))\n","    axes = axes.flatten()\n","    for img, ax, label in zip(images, axes, labels):\n","        ax.imshow(img)\n","        ax.set_title(label)\n","        ax.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","print(images_with_predictions)\n","\n","# Extract images and labels from the list\n","images, labels = zip(*images_with_predictions)\n","\n","# Plot the images with predictions\n","plot_images(images, labels, cols=4, rows=3)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1-Co_QcnIsAL3WLTHxvnnetemCZkEhWtz"},"id":"AKjiTARlQZaX","executionInfo":{"status":"ok","timestamp":1722230003192,"user_tz":-330,"elapsed":18915,"user":{"displayName":"Anju Doraisamy","userId":"07553379767866067180"}},"outputId":"52c94cd4-f24d-4e87-d8e7-fbe01584a6b4"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from mtcnn import MTCNN\n","from google.colab.patches import cv2_imshow\n","import tensorflow as tf\n","\n","# Function to expand the bounding box\n","def expand_bbox(bbox, expansion_factor=0.3):\n","    x, y, w, h = bbox\n","    new_w = int(w * (1 + expansion_factor))\n","    new_h = int(h * (1 + expansion_factor))\n","    new_x = max(0, x - (new_w - w) // 2)\n","    new_y = max(0, y - (new_h - h) // 2)\n","    return new_x, new_y, new_w, new_h\n","\n","# Function to preprocess face image for prediction\n","def preprocess_face(face_img, target_size=(224, 224)):\n","    face_img = cv2.resize(face_img, target_size)\n","    face_img = face_img / 255.0  # Normalize to [0, 1]\n","    face_img = np.expand_dims(face_img, axis=0)  # Add batch dimension\n","    return face_img\n","\n","# Initialize the face detector with adjusted parameters\n","detector = MTCNN(\n","    min_face_size=15,  # Adjust as needed\n","    scale_factor=0.5,  # Adjust as needed\n","    steps_threshold=[0.4, 0.4, 0.3]  # Lower values for more sensitivity\n",")\n","\n","# Load the trained model\n","model = tf.keras.models.load_model('/content/drive/My Drive/face_mask_detection_model_1pt4.h5')\n","class_names = ['with_mask', 'without_mask']\n","\n","# Define a confidence threshold for uncertain predictions\n","confidence_threshold = 0.40\n","\n","# Path to the video file or 0 for webcam\n","video_path = '/content/drive/My Drive/VID3.webm'\n","\n","# Open the video file\n","cap = cv2.VideoCapture(video_path)\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Detect faces in the frame\n","    faces = detector.detect_faces(frame)\n","    for face in faces:\n","        x, y, w, h = face['box']\n","        x, y, w, h = expand_bbox((x, y, w, h), expansion_factor=0.3)  # Adjust the expansion factor as needed\n","        face_img = frame[y:y+h, x:x+w]\n","\n","        if face_img.size == 0:\n","            continue\n","\n","        # Preprocess the face image\n","        face_img = preprocess_face(face_img)\n","\n","        # Predict using the trained model\n","        pred = model.predict(face_img)\n","        confidence = pred[0][0]\n","        class_idx = int(confidence > 0.5)\n","\n","        # Debug: print prediction\n","        print(f\"Prediction: {pred[0][0]}, Class Index: {class_idx}\")\n","\n","        # Check if prediction is uncertain\n","        if confidence > confidence_threshold and confidence < (1 - confidence_threshold) or confidence == None:\n","            label = f\"Uncertain: {confidence:.2f}\"\n","            color = (0, 255, 255)  # Yellow color for uncertain predictions\n","        else:\n","            # Get the class name\n","            class_name = class_names[class_idx]\n","            color = (0, 255, 0) if class_idx == 0 else (0, 0, 255)\n","            label = f\"{class_name}: {confidence:.2f}\"\n","\n","        # Draw bounding box and label on the frame\n","        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n","        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n","\n","    # Display the frame\n","    cv2_imshow(frame)\n","\n","    # Break the loop if 'q' is pressed\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","# Release the video file and close all OpenCV windows\n","cap.release()\n","cv2.destroyAllWindows()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1AmxUt_8l7BETagjRwABjPBXluPPhl3ml"},"id":"1Syk5d5pQEpB","executionInfo":{"status":"ok","timestamp":1722238215521,"user_tz":-330,"elapsed":452697,"user":{"displayName":"Anju Doraisamy","userId":"07553379767866067180"}},"outputId":"f5e6a62a-4600-4e31-8764-5972584305e4"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNIH5nJ84hWQbDzEyXL9Olq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}